Perfect — if you’re wiring **GA4 + Google Ads + Search Console** right now, add these too so the agent can actually explain “why” (not just “what happened”):

What to add next (highest value)

1. Search Console (must-have for organic drops)

* Use a **Domain property** if you have it: `sc-domain:empathyhealthclinic.com`
* If you only have URL-prefix, use the exact prefix: `https://empathyhealthclinic.com/`
* Also make sure the agent can pull:

  * Performance (queries + pages)
  * Indexing coverage signals (if available via API for your property type)
  * Sitemaps list + last submitted/read times

2. Google Ads (must-have for spend drop)
   Beyond spend/clicks, make sure you capture:

* Campaign + ad group status (paused/limited/budget)
* **Policy & disapprovals** summary (big one)
* **Change history** (this is the smoking gun when delivery suddenly drops)
* Conversion action status (recording / not recording) and primary vs secondary
* Billing / payments state (if accessible; otherwise at least surface “check billing” ticket)

3. GA4 (must-have for “is it real vs tracking broken”)
   Pull these slices so it can pinpoint where the drop came from:

* Sessions/users by default channel group
* Landing pages (top N) + sessions trend
* Device + geo
* Engagement + key events / conversions
* A “tag health” hint: do we still see realtime events at all?

Clarity (useful, but don’t block on it)
Clarity is great for “what users did once they arrived,” but it’s not usually the root-cause detector for traffic drops. Two ways to use it:

* Quick win: add a ticket rule that says “If GA4 traffic is down but Clarity sessions are steady (or vice versa), suspect tracking/tag issues.”
* Practical approach: if Clarity API access is messy, treat Clarity as **manual evidence links** in the report for now (top recordings, heatmaps), and automate later.

Other systems that help you “get the full picture”
If you can add any of these, the agent becomes dramatically better at root-cause:

1. Site crawl + indexability

* Robots.txt + sitemap.xml fetch (already in spec)
* Page fetch checks for top landing pages (already in spec)
* Add **HTTP response headers** capture (x-robots-tag, cache-control, redirects)

2. Server / deploy signals

* Deploy history timestamps (from your host: Vercel/Netlify/Render/Fly/etc.)
* Error rate / uptime (even a basic ping + status endpoint)
* If you have Cloudflare: cache status / firewall blocks / bot protection spikes

3. Search Console “Crawl stats” / indexing signals
   If accessible, this is huge for diagnosing crawler visibility vs content quality issues.

What I want you to do while you’re plugging things in
When you’re done, paste these 6 values exactly (no secrets, just identifiers):

* GA4_PROPERTY_ID
* GSC_SITE (exact string)
* ADS_CUSTOMER_ID
* DOMAIN
* GOOGLE_REDIRECT_URI (the one you configured)
* The list of scopes you enabled (or just say “default”)

Then we’ll know immediately if anything is mismatched (most common breakpoints: wrong GSC_SITE format, redirect URI mismatch, missing offline access so no refresh token, Ads customer ID formatting).
