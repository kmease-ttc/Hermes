Here’s exactly how to turn this Integrations page into a real “nervous system” that shows **last run**, **what it did**, **for which site**, and **what it pulled back**—and remove the junk UI so it only shows what you need.

---

## What you need the page to answer (in 10 seconds)

For each service, you should be able to see:

1. **Last Run**

* When did it last execute successfully (and last attempt)?
* For which site?
* Was it scheduled or manual?
* Status: success / partial / failed / skipped

2. **What it did**

* Which inputs it used (site, URLs, date range, settings)
* What it called (provider endpoints, external APIs)
* What it produced (counts + a short summary)
* What it stored (tables updated, artifacts created)

3. **What it should have done**

* A checklist of expected outputs for that service (per run)
* Comparison: expected vs actual (missing outputs highlighted)

4. **Drill-down**

* Click into a run detail view (summary first, with deeper details available)

---

## Developer Instructions: Implement “Runs” + “Run Details” (truthful + actionable)

### 1) Create a canonical “Service Runs” model

You need a single table that every service writes to whenever it runs.

**Table: `service_runs`**

* `id` (uuid)
* `site_id` (uuid)
* `site_domain` (string snapshot)
* `service_id` (uuid / stable slug)
* `service_name` (string snapshot)
* `trigger` (“scheduled” | “manual” | “webhook”)
* `status` (“success” | “partial” | “failed” | “skipped”)
* `started_at` (timestamp)
* `finished_at` (timestamp)
* `duration_ms` (int)
* `version` (string: git sha or build version if available)
* `summary` (text, short)
* `metrics_json` (jsonb) ← counts, urls crawled, rows written, etc.
* `inputs_json` (jsonb) ← run config used
* `outputs_json` (jsonb) ← expected/actual results list
* `error_code` (string nullable)
* `error_detail` (text nullable)

**Key point:** this table is *owned by Hermes* and is the truth layer.

---

### 2) Standardize what each service must report (Expected vs Actual)

Add a per-service “expected outputs checklist” into your `services` inventory record (or a separate table).

**Example expected outputs fields**

* `expected_outputs: ["rows_written:seo_issues", "artifact:sitemap", "metric:pages_scanned"]`

Then every run writes `outputs_json` like:

```json
{
  "expected": ["seo_issues", "crawl_summary", "broken_links"],
  "actual": ["crawl_summary"],
  "missing": ["seo_issues", "broken_links"]
}
```

This gives you exactly what you asked for: “what it should do vs what it did.”

---

### 3) Add API endpoints Hermes needs

#### A) List services with last run info

`GET /api/services/with-last-run?site_id=...`
Returns each service + last run summary fields:

* last_run_started_at
* last_run_finished_at
* last_run_status
* last_run_site_domain
* last_run_summary
* last_run_metrics (small subset)

#### B) List runs for a service (by site)

`GET /api/services/:serviceId/runs?site_id=...&limit=25`

#### C) Run detail view

`GET /api/runs/:runId`
Returns:

* summary
* inputs_json
* metrics_json
* outputs_json (expected/actual/missing)
* error detail
* links to artifacts / logs

#### D) Link from integrations table “View last run”

Add a button/icon that opens the run detail modal or navigates to:
`/runs/:runId`

---

## UI Changes: Make it useful and remove useless items

### 1) Service Inventory table columns (final set)

Keep only columns that help you operate:

* **Service**
* **Last Run** (time + status pill)
* **Site** (domain)
* **Summary** (1 line)
* **Key Metrics** (small: e.g. “42 issues”, “112 pages”, “3 alerts”)
* **Missing Outputs** (badge count, clickable)
* **Actions**: View, Runs, Test

Remove these from the main table (they’re currently noise):

* “Configured”
* “Reachable”
* “/health”
* “Auth”
* “Secret”
* “E2E”

Those should be in a compact “Service Diagnostics” drawer, not front-and-center.

**Why:** You’re not trying to be a DevOps dashboard. You’re trying to know “did it run, and did it do what it’s supposed to do?”

---

### 2) Add a “Service Diagnostics” drawer (only when needed)

When you click a service row (or a small “Diagnostics” icon), show:

* Base URL
* Configured ✅/❌
* Reachable ✅/❌
* Health ✅/❌
* Auth ✅/❌
* Secret ✅/❌
* E2E ✅/❌
* Last checked time
* Error detail

This keeps the page clean but still gives you debugging power.

---

### 3) The run detail view you asked for (summary first)

When you open a run, show:

Header

* Service name
* Status + duration
* Site domain
* Trigger
* Started/finished time

Summary block

* 2–6 bullet summary (generated from `summary` + metrics)

Inputs (collapsed)

* site_id
* urls
* date range
* settings flags

Outputs (most important)

* Expected outputs list
* Actual outputs list
* Missing outputs highlighted

Artifacts/links (optional)

* link to a stored report
* link to a crawl export
* link to SERP snapshot
* link to logs

---

## How services should write “runs” (implementation pattern)

Every worker call (or internal job) should:

1. Create a run record at start: status=running
2. As it progresses, update metrics_json incrementally
3. On completion:

* set status success/partial/failed
* set summary
* set outputs_json expected/actual/missing
* attach error detail if failed

If you don’t have a centralized runner yet, Hermes can still log a run when it calls a service endpoint.

---

## Make it immediately valuable even if workers don’t report yet (bridge approach)

If workers aren’t logging runs themselves yet:

* Hermes can log a “proxy run” whenever it triggers a job or test call:

  * “Called /crawl?site=…”
  * capture response summary and store in run record
  * this is still hugely useful right away

---

## What to do next (short execution plan)

1. Add `service_runs` table + run APIs (above)
2. Update Integrations page:

   * show Last Run, Site, Summary, Metrics, Missing Outputs
   * remove noisy columns
3. Add run detail modal/view
4. Update each service (or Hermes proxy) to write run records

---

If you want me to be extremely specific, paste the list of “what each service is supposed to pull back” (even rough bullet points per service), and I’ll produce:

* the `expected_outputs` checklist per service
* the exact `metrics_json` schema per service
* the first-pass “summary generator” rules so the summaries are consistent and actually useful.
