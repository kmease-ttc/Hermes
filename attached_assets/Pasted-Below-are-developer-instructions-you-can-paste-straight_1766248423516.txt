Below are developer instructions you can paste straight into Replit (or hand to your dev) to build the “SEO Doctor Orchestrator” so it works for 4 sites today and N sites later, with safe deployment guardrails and a regular cadence.

---

## SEO Doctor Orchestrator — Developer Instructions

### Goal

Build an “orchestrator” platform that can:

1. Continuously diagnose SEO + performance + analytics issues across any number of configured websites
2. Produce prioritized fix plans with evidence
3. Generate machine-applicable patches (or PRs) for the correct codebase
4. Run validation checks
5. Deploy safely on a controlled cadence with clear guardrails and rollback

The system must support adding new sites without code changes (config-driven), and must avoid “editing production blindly.”

---

## Core Concepts

### 1) Site Registry (Config-Driven)

Create a `sites` registry stored in DB (preferred) or JSON config (acceptable MVP) that defines each managed site.

Each site record should include:

* `site_id` (string, unique)
* `display_name`
* `base_url`
* `category` (clinic, SEO tool, property mgmt, farm shop, etc. — used only for reporting)
* `tech_stack` (enum: nextjs, remix, react-static, wordpress, webflow, unknown)
* `repo_provider` (enum: github, replit, other)
* `repo_identifier` (e.g., GitHub org/repo OR Replit project id)
* `deploy_method` (enum: replit_deploy, vercel, netlify, cloudflare_pages, manual)
* `crawl_settings`

  * `crawl_depth_limit`
  * `max_pages`
  * `respect_robots` (default true)
  * `user_agent`
* `sitemaps`

  * `sitemap_urls[]` (optional; if empty, auto-discover)
* `key_pages[]` (login, pricing, contact, location pages, etc.)
* `integrations`

  * `ga4.property_id` (optional)
  * `gsc.property` (optional)
  * `google_ads.customer_id` (optional)
  * `clarity.site_id` (optional)
* `guardrails`

  * `allowed_edit_paths[]` (glob patterns)
  * `blocked_edit_paths[]`
  * `max_files_changed_per_run` (default 20)
  * `max_lines_changed_per_run` (default 800)
  * `require_human_approval` (bool)
  * `auto_merge_categories[]` (low risk only)
* `cadence`

  * `diagnose_frequency` (e.g., daily)
  * `auto_fix_frequency` (e.g., weekly)
  * `content_frequency` (e.g., weekly or biweekly)
  * `quiet_hours` (time window where no deploys happen)
* `owner`

  * `owner_name`
  * `slack_or_email` (for notifications)

UI requirement: a simple “Add Site” page that creates a record and runs an onboarding checklist.

---

## Modules

### 2) Diagnoser (Evidence-Based Findings)

Implement a diagnoser pipeline that produces structured findings. Output must be JSON.

#### Inputs

* Site config
* Crawl results
* Optional: GA4/GSC/Ads data snapshots

#### Outputs (Finding model)

Each finding should be a record:

* `finding_id`
* `site_id`
* `category` (crawlability, indexation, content, performance, structured_data, internal_links, analytics, ads, security_headers)
* `severity` (critical/high/medium/low)
* `impact_score` (0–100)
* `confidence` (0–1)
* `title`
* `description`
* `evidence[]`

  * `{type: "url"|"metric"|"screenshot"|"html_snippet"|"header"|"log", value: "..."}`
* `recommended_actions[]` (IDs referencing actions)
* `created_at`
* `status` (open, accepted, fixed, ignored)

Minimum diagnoser checks (MVP):

* Crawlability:

  * robots.txt validity and blocking rules
  * sitemap existence + parse success
  * status codes (4xx/5xx)
  * redirect chains
  * canonical consistency
  * noindex/nofollow usage
* On-page:

  * missing/duplicate title tags
  * missing/duplicate meta descriptions
  * missing H1 / multiple H1
  * thin pages (low word count)
* Internal linking:

  * orphan pages
  * pages without internal outlinks
* Performance (lightweight):

  * collect core web vitals via PageSpeed API if available later; MVP can do TTFB + asset size checks
* Structured data:

  * basic schema presence and JSON-LD parseable
* Analytics instrumentation:

  * GA4 tag presence (if expected)
  * event capture sanity checks (optional)

Store crawl outputs and findings in DB with historical snapshots so you can track trends.

---

### 3) Prioritizer (Roadmap Builder)

Create a prioritizer that groups findings into a plan:

Plan output:

* `plan_id`, `site_id`
* `generated_at`
* `buckets[]`:

  * `Quick wins (low effort / high impact)`
  * `Structural fixes (templates, routing, canonical rules)`
  * `Content opportunities (new pages, rewrite sections)`
  * `Technical debt (performance, refactors)`
* Each item includes:

  * `finding_id`
  * `effort_estimate` (S/M/L)
  * `risk_level` (low/medium/high)
  * `recommended_execution_mode` (auto_patch, PR, manual)

Rule: only allow `auto_patch` for low-risk items.

---

### 4) Patch Generator (Safe, Machine-Applicable)

The orchestrator should not “freehand edit prod.” It generates changes in one of these formats:

#### Option A (Preferred): Git PR workflow

* Create branch
* Apply patch to allowed files
* Commit with message and summary
* Open PR
* Run checks
* Auto-merge only if:

  * risk is low
  * checks pass
  * site guardrails allow it

#### Option B: Replit patch workflow

* Apply patch to repo in Replit
* Run build/test
* Deploy preview (if supported)
* Promote to prod per cadence + guardrails

Patch record should include:

* `patch_id`, `site_id`, `plan_id`
* `changes[]`:

  * `{file_path, diff_unified}`
* `rationale` (short)
* `acceptance_criteria[]` (must be testable)
* `risk_level`
* `created_at`
* `status` (queued, applied, failed, deployed, rolled_back)

Guardrails for patch application:

* Verify file paths match `allowed_edit_paths` and not `blocked_edit_paths`
* Enforce `max_files_changed_per_run` and `max_lines_changed_per_run`
* Block changes to auth/billing/critical runtime config by default unless explicitly allowed

---

### 5) Validation & Smoke Tests

Before any deploy, run:

Build checks:

* `npm test` if present
* `npm run lint` if present
* `npm run build` (must pass)

SEO sanity checks (automated):

* Fetch:

  * `/robots.txt` (200)
  * sitemap(s) (200 + parseable)
  * a sample of key pages (200, canonical present)
* Ensure no accidental:

  * sitewide `noindex`
  * broken canonical pointing to staging
  * blocked crawling for main pages

If any fails → stop and mark patch failed.

After deploy:

* Repeat same checks against production URL
* Optional: run a mini-crawl of 20–50 pages

Rollback:

* Keep last known-good deploy artifact or commit SHA
* One-click rollback: revert commit / redeploy previous version

---

## Cadence & Guardrails (How often updates happen)

### 6) Scheduling Rules

Implement scheduling in two layers:

1. Diagnostics (safe to run frequently)

* Default: daily for each site
* Also allow “Run now” manually

2. Automated fixes (risk-managed)

* Default: weekly
* Only applies low-risk categories:

  * meta tag fixes
  * robots/sitemap fixes (careful)
  * broken internal link updates (simple)
  * template-level missing H1 fixes (only if known safe)
* Structural routing changes, canonical logic changes, large content rewrites:

  * PR only + approval required

3. Content generation / new pages

* Default: weekly or biweekly per site
* Must go through PR/approval (unless you explicitly allow auto-merge on certain sites)

### 7) Deployment Guardrails

* No deploys during `quiet_hours`
* Rate limit:

  * max 1 production deploy per site per 24 hours (configurable)
* Change budget:

  * low-risk patches only for auto deploy
* If a site has 2 consecutive failed deploys → freeze auto-fix until manual review

---

## UI Requirements (MVP)

Build a simple dashboard:

### Sites

* List sites
* Add site
* Edit site config
* Show last diagnosis time, last deploy time, current health score

### Findings

* Filter by site, severity, category
* Click a finding → see evidence and suggested actions
* Buttons:

  * “Generate patch”
  * “Create PR”
  * “Ignore” (requires reason)

### Plans

* Generated roadmap by site
* “Apply low-risk fixes (weekly batch)” button

### Deployments

* History of patches/PRs/deploys
* Status + logs + rollback button

---

## Data Model (Minimum Tables)

* `sites`
* `crawl_runs`
* `findings`
* `plans`
* `patches`
* `deployments`
* `audit_logs` (who/what/when)

Everything should be auditable.

---

## Implementation Notes (Pragmatic)

* Start with a single “crawler” implementation (headless fetch + parse links) for MVP.
* Use adapters:

  * `CrawlAdapter`
  * `RepoAdapter` (GitHub/Replit)
  * `DeployAdapter`
  * `AnalyticsAdapter` (GA4/GSC/Ads)
* That keeps you from hardcoding assumptions per site.

---

## Definition of Done (MVP)

1. Add a site via UI, run onboarding, and store config
2. Nightly diagnosis produces findings with evidence
3. Generate a plan with prioritization
4. Generate a patch for at least 3 common fix types (titles/meta/H1/robots/sitemap)
5. Apply patch via PR or Replit deploy with checks
6. Track history and rollback

---

## Optional Next Phase (After MVP Works)

* Trend charts: impressions/clicks/rank changes by page (GSC)
* Automatic “drop detection” alerts (traffic or spend falls off)
* Programmatic internal linking suggestions
* Programmatic schema improvements
* A/B testing suggestions and safe rollout flags

---

If you want, paste what each of your current four sites is built with (Next.js / static / WP / etc.) and whether they’re GitHub repos or Replit-native, and I’ll tailor the `RepoAdapter` + `DeployAdapter` section into an exact implementation plan (including how to structure the patch format and which fix categories are safe for auto-merge).
