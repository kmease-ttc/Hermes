Perfect. Let’s turn Screaming Frog into a “fake API” you can call from SeoScheduler.

High-level:

* Screaming Frog SEO Spider installed on a box you control (Mac mini / Windows / Linux VM).
* Small Python FastAPI app on that same box.
* FastAPI calls the Screaming Frog CLI, watches for exports, and returns data as JSON.
* Your Replit SeoScheduler hits that FastAPI over HTTPS.

---

## 1. Screaming Frog CLI basics

On the machine where Screaming Frog is installed, you can already do something like:

```bash
ScreamingFrogSEOSpiderCli \
  --crawl https://example.com \
  --headless \
  --save-crawl \
  --output-folder /tmp/sf_exports \
  --export-tabs "Internal:All" \
  --export-format csv
```

Adjust the binary name/path as needed:

* macOS: `/Applications/Screaming Frog SEO Spider.app/Contents/MacOS/ScreamingFrogSEOSpiderCli`
* Windows: `"C:\Program Files\Screaming Frog SEO Spider\ScreamingFrogSEOSpiderCli.exe"`

---

## 2. Minimal API wrapper (FastAPI)

Create a new folder on the Screaming Frog box, e.g. `sf-api/` with:

### `app.py`

```python
import os
import uuid
import subprocess
from pathlib import Path
from fastapi import FastAPI
from pydantic import BaseModel
from typing import Optional
import csv
import json

# 1. CONFIG
SF_CLI_PATH = os.getenv(
    "SF_CLI_PATH",
    "/Applications/Screaming Frog SEO Spider.app/Contents/MacOS/ScreamingFrogSEOSpiderCli",
)
EXPORT_ROOT = Path(os.getenv("SF_EXPORT_ROOT", "/tmp/sf_exports"))
EXPORT_ROOT.mkdir(parents=True, exist_ok=True)

app = FastAPI(title="Screaming Frog API Wrapper")

# 2. REQUEST MODELS
class CrawlRequest(BaseModel):
    url: str
    export_tab: str = "Internal:All"  # Screaming Frog tab
    export_format: str = "csv"
    max_pages: Optional[int] = None   # optional cap

class CrawlResponse(BaseModel):
    job_id: str
    status: str

# 3. HELPER
def run_sf_crawl(job_id: str, req: CrawlRequest) -> Path:
    job_dir = EXPORT_ROOT / job_id
    job_dir.mkdir(parents=True, exist_ok=True)

    cmd = [
        SF_CLI_PATH,
        "--crawl", req.url,
        "--headless",
        "--save-crawl",
        "--output-folder", str(job_dir),
        "--export-tabs", req.export_tab,
        "--export-format", req.export_format,
    ]

    if req.max_pages:
        cmd.extend(["--max-pages", str(req.max_pages)])

    # Run synchronously for now (simple version)
    subprocess.run(cmd, check=True)

    # Screaming Frog typically names exports like:
    #  {export_tab}.csv (e.g., "Internal_All.csv"). You may need
    # to tweak this to match your actual filenames.
    export_file = next(job_dir.glob("*.csv"))
    return export_file

# 4. ENDPOINTS

@app.post("/crawl", response_model=CrawlResponse)
def crawl_site(req: CrawlRequest):
    job_id = str(uuid.uuid4())
    try:
        export_file = run_sf_crawl(job_id, req)
        # Mark success with a simple flag file
        (EXPORT_ROOT / job_id / "DONE").write_text(export_file.name)
        status = "completed"
    except Exception as e:
        (EXPORT_ROOT / job_id / "ERROR").write_text(str(e))
        status = "error"

    return CrawlResponse(job_id=job_id, status=status)

@app.get("/status/{job_id}")
def crawl_status(job_id: str):
    job_dir = EXPORT_ROOT / job_id
    if not job_dir.exists():
        return {"job_id": job_id, "status": "not_found"}

    if (job_dir / "ERROR").exists():
        return {
            "job_id": job_id,
            "status": "error",
            "error": (job_dir / "ERROR").read_text(),
        }

    if (job_dir / "DONE").exists():
        return {
            "job_id": job_id,
            "status": "completed",
            "export_file": (job_dir / "DONE").read_text().strip(),
        }

    return {"job_id": job_id, "status": "running"}

@app.get("/results/{job_id}")
def crawl_results(job_id: str):
    job_dir = EXPORT_ROOT / job_id
    done_file = job_dir / "DONE"
    if not done_file.exists():
        return {"job_id": job_id, "status": "not_ready"}

    export_name = done_file.read_text().strip()
    export_path = job_dir / export_name
    if not export_path.exists():
        return {"job_id": job_id, "status": "missing_export"}

    # Parse CSV → JSON
    with export_path.open(newline="", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        rows = list(reader)

    return {
        "job_id": job_id,
        "status": "completed",
        "row_count": len(rows),
        "data": rows,
    }
```

### Run the API

```bash
pip install fastapi uvicorn
export SF_CLI_PATH="/path/to/ScreamingFrogSEOSpiderCli"
export SF_EXPORT_ROOT="/tmp/sf_exports"

uvicorn app:app --host 0.0.0.0 --port 8080
```

(Front this with nginx + HTTPS when you’re happy.)

---

## 3. How SeoScheduler would call it

In your Replit `daily_seo_pipeline.py`, add something like:

```python
import requests

SF_API_BASE = "https://your-sf-box.example.com"  # or local IP/VPN

def run_screaming_frog_crawl(url: str):
    # 1. Kick off crawl
    resp = requests.post(
        f"{SF_API_BASE}/crawl",
        json={"url": url, "export_tab": "Internal:All", "export_format": "csv"},
        timeout=5,
    )
    resp.raise_for_status()
    job = resp.json()
    job_id = job["job_id"]

    # 2. (Simple version) immediately fetch results – because the wrapper runs synchronously
    results = requests.get(f"{SF_API_BASE}/results/{job_id}", timeout=60)
    results.raise_for_status()
    return results.json()

# Example use in pipeline
def audit_site_with_sf(domain: str):
    data = run_screaming_frog_crawl(f"https://{domain}")
    rows = data["data"]
    # Now integrate with your existing tasks_* CSV flow
    # e.g. build tasks for 4xx, 5xx, long titles, missing H1, etc.
    return rows
```

If you later want long crawls to be async:

* Change `crawl_site` to spawn a background thread/process instead of `subprocess.run(..., check=True)`.
* Poll `/status/{job_id}` from SeoScheduler until `completed`, then hit `/results/{job_id}`.

---

## 4. How this plugs into your current pipeline

Given what you already have in SeoScheduler (SERP checks, task CSVs, implementation verification), this wrapper lets you:

* Add a “technical crawl” step per domain.
* Parse output into `tech_audit.csv` or merge into `tasks_with_tech.csv`.
* Auto-generate tasks like:

  * Fix 5xx/4xx URLs
  * Compress large images
  * Reduce long titles / meta descriptions
  * Fix duplicate URLs

If you want, next step I can give you:

* A concrete parser that maps Screaming Frog `Internal:All` columns → your existing `tasks_*` schema so it drops straight into your current CSV/task pipeline.
