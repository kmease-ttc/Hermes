Implement a **Connector Diagnostics Pipeline** inside Hermes for each integration/service (starting with Google Data Connector).

Goal: When a user runs “Test Connection” or “Run Smoke Test”, show a **stage-by-stage pipeline** with pass/fail + debug info so the user can immediately see where it broke.

### UI: Services → Service Details → Diagnostics

Add a “Diagnostics” panel for each service with a vertical pipeline of stages:

1. Config Loaded
2. Auth Ready
3. Endpoint Built
4. Request Sent
5. Response Type Validated
6. Schema Validated
7. UI Mapping (optional)

Each stage shows:

* Status: Pending / Pass / Fail
* Timestamp and duration
* A short message
* Expandable details (debug-only)

### Stage Definitions + Data Captured

1. Config Loaded

* Pass if service config resolves successfully
* Capture: config keys present (redact values), resolved base_url host, auth mode, required metrics list
* Fail if missing keys or parsing error

2. Auth Ready

* Pass if auth requirements are satisfied for this service
* For OAuth services: token present + not expired (or refresh ok)
* For API-key services: key present (redacted)
* Capture: auth type, token status (no token value), scopes (if available), expiry time (if available)

3. Endpoint Built

* Pass if endpoint URL is constructed
* Capture: full URL (safe), method, headers list (redact secrets), query params

4. Request Sent

* Pass if request completes (even with non-200)
* Capture: network timing, status code, error type if transport failed

5. Response Type Validated

* Pass if content-type is JSON AND body parses as JSON (when JSON expected)
* Fail if content-type is text/html or JSON parse fails
* Capture: response content-type, first 200 chars of body, response headers (safe)

6. Schema Validated

* Pass if expected output fields exist (based on the integration’s required metrics)
* Fail if missing keys
* Capture: missing keys list, present keys list (top-level), any provider error messages

7. UI Mapping (optional)

* Pass if backend payload matches frontend expectations
* Capture: validation result / mismatch details

### Storage + Persistence

* Persist last N diagnostic runs per service in the DB (e.g. 20 runs)
* Surface the latest run summary on the Services page
* Provide a “Copy diagnostics” button that copies a redacted JSON debug blob (no secrets)

### Error Handling Requirements

* Never throw raw HTML/doctype parsing errors to the user without also showing:

  * status code
  * content-type
  * called URL
  * response snippet
* Ensure redaction of secrets in logs and UI.

### Implementation Notes

* Centralize diagnostics capture in the smoke test runner so all services benefit.
* Different services can declare:

  * expected response type (json/html)
  * required output fields
  * auth mode (oauth/api-key/none)
* Default: expected JSON, required fields empty.

Deliverable: A working Diagnostics panel for at least the Google Data Connector that clearly shows where failures happen.
