Got it. You’re asking for **self-describing services**, so the Integrations page can render everything it needs **without you copying/pasting descriptions or summaries**, and so your developer agent can generate it automatically.

Below are **precise developer instructions** to make that happen.

---

# Developer Instructions: Make Integrations Self-Describing (No Copy/Paste)

## Goal

The Integrations page should be able to render:

* Service name
* Service description (explainer)
* “What this service does”
* “What it last did”
* “What it returned”
* “What’s missing / broken”

**All from structured metadata and run records**, not manually pasted text.

---

## 1) Introduce a canonical `service_catalog` (source of truth)

Create a **single machine-readable catalog** that defines every service once.

### Location (choose one, not both)

* `shared/serviceCatalog.ts` (recommended)
* OR a DB table `service_catalog`

This catalog is what *both* Hermes and the UI read from.

---

## 2) Required fields per service (this is the key part)

Each service entry **must include summary slugs**, not prose.

### Example schema (TypeScript)

```ts
export interface ServiceDefinition {
  slug: string;                 // "crawl_render"
  displayName: string;          // "Crawl & Render Service"
  category: "worker" | "connector" | "planned" | "dependency";

  description: string;          // 2–3 sentence explainer (shown in UI)

  purpose: string;              // one-liner: "Detect technical SEO issues via crawl + render"

  inputs: string[];             // summary slugs
  outputs: string[];            // summary slugs
  keyMetrics: string[];         // summary slugs
  commonFailures: string[];     // summary slugs

  runTriggers: string[];        // "scheduled", "manual", "on_change"
}
```

---

## 3) What you mean by “summary slugs” (important)

A **summary slug** is a short, stable identifier that can be:

* rendered as UI text
* compared against run outputs
* flagged as missing
* aggregated across runs

### Example summary slugs

```ts
inputs: [
  "site_domain",
  "crawl_scope",
  "robots_txt",
  "sitemap_xml"
],

outputs: [
  "pages_crawled",
  "indexable_pages",
  "non_200_urls",
  "canonical_errors",
  "render_failures"
],

keyMetrics: [
  "total_pages",
  "error_count",
  "warning_count"
],

commonFailures: [
  "blocked_by_robots",
  "js_render_blank",
  "timeout"
]
```

These are **what the UI uses**, not free text.

---

## 4) How the Integrations page should render automatically

### Service list view

From `serviceCatalog`:

* Service name
* Purpose (one-liner)

From `service_runs` (latest run):

* Last run time
* Status
* Metrics (mapped by slug)
* Missing outputs (computed)

No manual text.

---

### Service detail view (when clicked)

The UI should render **five sections**, all auto-generated:

#### A) What this service does

* `description`
* `purpose`

#### B) What it expects to do

Render lists from catalog:

* Inputs
* Outputs
* Key metrics

#### C) What it last did

From latest `service_run`:

* summary
* site
* duration
* trigger

#### D) What it returned

Compare:

* `expected outputs` (catalog)
* vs `actual outputs` (run)

Highlight missing slugs.

#### E) What went wrong (if applicable)

* Map `run.error_code` to `commonFailures` slugs

---

## 5) Change how runs are written (this removes guesswork)

### When a service runs, it must emit **slugs**, not prose

#### Example `service_run.outputs_json`

```json
{
  "actualOutputs": [
    "pages_crawled",
    "indexable_pages",
    "canonical_errors"
  ],
  "metrics": {
    "pages_crawled": 278,
    "canonical_errors": 14
  }
}
```

Hermes does **not** need to interpret text.
The UI just matches slugs.

---

## 6) Computing “what’s missing” (automatic)

On the backend (or frontend):

```ts
missingOutputs =
  serviceCatalog.outputs
    .filter(o => !run.actualOutputs.includes(o));
```

That’s it.

No copy/paste. No guessing.

---

## 7) Test Connection button (same pattern)

When “Test Connection” runs:

* Write a `service_run`
* Populate:

  * `actualOutputs` with whatever was verified
  * `metrics` with lightweight counts
* Status = `success | failed | skipped`

The UI already knows how to render it.

---

## 8) What to remove from the UI (since it’s now redundant)

Once this is in place, **remove** from the default view:

* base URL
* configured
* health
* auth
* secret
* E2E

Those belong in a collapsible **Diagnostics** panel only.

---

## 9) Acceptance criteria (this is how you’ll know it worked)

You should be able to:

* Add a new service by **only editing `serviceCatalog`**
* Run it once
* See:

  * description
  * expected inputs/outputs
  * last run summary
  * metrics
  * missing outputs

With **zero copy/paste**.

---

## 10) What to tell your developer agent verbatim

You can hand them this exact instruction:

> “Make the Integrations page self-describing by introducing a canonical `serviceCatalog` that defines description, purpose, inputs, outputs, and metrics as summary slugs. Services must emit output slugs in `service_runs`. The UI should render expectations vs actuals by comparing catalog slugs to run slugs. No service descriptions or summaries should be manually pasted.”

---

If you want, next I can:

* Generate the **full `serviceCatalog` object** for all your existing services
* Or map your current services → correct input/output slugs so the agent can implement it directly without design decisions

Just say which one.
