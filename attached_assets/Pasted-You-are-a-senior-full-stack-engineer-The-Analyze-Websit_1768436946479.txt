You are a senior full-stack engineer. The **Analyze Website workflow currently renders placeholder data**. We must replace all placeholder/fixture values with real data from the canonical crew identity + ingestion pipeline.

Goal
When a user runs “Analyze”, the system must:

1. Run the correct crews (Scotty/Speedster/etc.) against the selected site
2. Store results in DB keyed by canonical `crewId`
3. Render the report using those stored results (no fake numbers)
4. Show clear “no data yet / run diagnostics” states instead of placeholders

Non-negotiables

* No “mock” or “sample” KPI values for ACTIVE crews.
* Every metric displayed must come from DB (`crew_runs`, `crew_kpis`, `crew_findings`) keyed by `crewId`.
* The Analyze Report page must only read from canonical APIs (overview/report endpoints), not worker calls directly.
* If a crew fails, the report must show failure reason from Integrations/outputs contract (not silence).

---

## Step 1 — Identify where placeholder data is coming from

1. Locate the Analyze flow route and pages, likely:

* `client/src/pages/analyze/*`
* `client/src/pages/report/*`
* `client/src/pages/scan/*`
* or `client/src/pages/SiteScan.tsx`

2. Search for placeholder sources:

* `mock`, `fixture`, `sampleData`, `demoData`, `placeholder`, `fake`, `static`
* hardcoded numbers like `12400`, `3.2`, `42%`, `18%`
* “Sample” badges that are shown even for active/connected crews

3. Produce a short list:

* file paths + component names + what they currently render

---

## Step 2 — Replace “Analyze” with a real run pipeline (server-side)

Create or fix the “Analyze” endpoint so it triggers real worker runs and persists results.

### A) API endpoint

Implement:

* `POST /api/analyze/run`
  Body:
* `siteId` (or domain)
* `crewIds?: CrewId[]` (optional; default is standard set)

Behavior:

1. Resolve the siteId/domain to a canonical `siteId`.
2. Determine the default crews for analyze:

   * scotty (technical)
   * speedster (web vitals)
   * sentinel (decay) if analytics available
   * hemingway (content) if crawl data available
   * popular (analytics) if configured
   * lookout/beacon/natasha as optional
3. For each crewId:

   * call `POST /api/crew/:crewId/run` (existing canonical run endpoint)
   * do not call workers directly here
4. Return a run summary:
   {
   ok: true,
   siteId,
   runs: [
   { crewId, status: "success"|"failed"|"skipped", primaryKpi?: {kpiId,value}, error?: string }
   ]
   }

Important:

* “skipped” if integration not configured (needs API key/base URL), with reason.

### B) Polling / job handling (keep simple)

If runs are slow:

* return immediately with `jobId`
* implement `GET /api/analyze/status?jobId=...`
* client polls every 2s until complete
  If runs are fast enough, synchronous is OK for now.

---

## Step 3 — Canonical report data endpoint (single source of truth)

Implement:

* `GET /api/analyze/report?siteId=...`

This endpoint must:

1. Read the latest successful run per crew from DB.
2. Assemble the report sections from real data:

   * Top summary (grade/health) derived from Scotty + Speedster KPIs
   * Key Metrics (Popular KPI etc.)
   * Capabilities & Performance grid (each crew’s primary KPI)
   * Findings (from crew_findings)
   * Recommended Actions (from tasks/missions if implemented)
3. Return a report JSON payload that the UI can render directly.

Rule:

* If a crew has no stored run, include:

  * status: "no_data"
  * message: "Run diagnostics to populate"
* If a crew is not configured, include:

  * status: "needs_config"
  * message: "Connect integration to enable"
* Never substitute placeholder values.

---

## Step 4 — UI: wire Analyze button to real pipeline

Update Analyze UI flow:

### On “Analyze My Website”

1. Call `POST /api/analyze/run`
2. Show progress UI:

   * list crews running with status
3. When complete, navigate to:

   * `/app/analyze/report?siteId=...` (or existing report route)
4. Report page calls:

   * `GET /api/analyze/report?siteId=...`
5. Render sections using returned data only.

Remove:

* local mock report data
* placeholder metric values

---

## Step 5 — Enforce “no placeholder for active crews”

In UI components that render KPI cards:

* If crew status is Active AND report says `no_data`, show:

  * “No data yet — Run diagnostics”
* If crew status is Active AND report includes a KPI, show it
* If crew status is Locked/needs_config, allow “Sample” values
  But:
* Sample values must be clearly labeled “Sample”
* Sample must never appear for Active crews

---

## Step 6 — Fix the “Scotty KPI missing” issue as part of this

Make sure Scotty’s run normalization writes:

* `technicalHealthScore` into crew_kpis

Because the Analyze report relies on those primary KPIs.

---

## Step 7 — Tests (required)

Add tests that fail if the analyzer uses placeholder data:

1. Unit test `GET /api/analyze/report`:

* When DB has no crew_kpis, response must contain `no_data` statuses (not numbers)
* When DB has KPIs, those exact values appear

2. Contract test:

* For each crew in crewRegistry, verify:

  * after a mocked run, primaryKpi.id exists in crew_kpis

3. UI test (minimal):

* Render report page with API returning `no_data`
* Assert it shows “No data yet” not numbers

---

## Deliverables

* List of placeholder sources removed (file paths)
* Working `/api/analyze/run` + `/api/analyze/report`
* Analyze button runs real crews and populates report
* Report displays real KPIs/findings or explicit no_data/needs_config states
* Tests added that prevent placeholder regressions
