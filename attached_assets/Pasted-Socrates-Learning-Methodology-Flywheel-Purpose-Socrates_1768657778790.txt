Socrates Learning Methodology (Flywheel)

Purpose
Socrates is the system’s memory and reasoning layer. Its job is to turn observed outcomes (breaks, regressions, improvements, big jumps) into durable “learnings” that improve future agent decisions. The flywheel is:

Agents act → outcomes observed → outcomes attributed to actions → learning created → learning retrieved during future actions → better decisions → fewer failures / faster wins.

Core Principles

1. Evidence-first: Socrates only stores learnings that are tied to observable evidence (logs, diffs, metrics, time windows).
2. Attribution-aware: A learning must include why we believe it’s related to an agent action (correlation + plausibility), not just that it happened.
3. Scoped applicability: A learning must define when it applies (site type, stack, page type, metric, environment).
4. Retrieval is mandatory: If a learning is relevant to an agent’s plan, the agent must cite it and comply with it unless it escalates.
5. Continuous refinement: Learnings can be superseded or merged; low-confidence learnings can be promoted/demoted based on repeated confirmation.

Definitions

* Event: A detected change in metrics, errors, or behavior (e.g., DA jump, crawl drop, LCP regression, indexing shift).
* Action: A discrete agent action that could affect outcomes (code change, config update, content change, deploy, integration setup).
* Observation Window: Time range used to link actions to events (e.g., 0–6 hours post-deploy for CWV changes).
* Attribution: A scored link between one or more actions and an event.
* Learning: A structured KB entry created when attribution crosses a confidence threshold.
* Policy: A “must/never” rule derived from learnings (e.g., never ship X without Y).

System Data Objects (Minimum Schema)

1. AgentActionLog (append-only)
   Each agent action produces a log entry:

* actionId
* agentId (Scotty, Beacon, Draper, etc.)
* siteId
* env (dev/prod)
* timestampStart / timestampEnd
* actionType (crawl, deploy, content_update, config_change, integration_setup)
* targets (urls, files, configs, service endpoints)
* diffSummary (if code/content changed)
* commitSha / deployId (if applicable)
* jobId / runId
* expectedImpact (metrics expected to move)
* riskLevel (low/med/high)
* notes

2. OutcomeEventLog (append-only)
   The system continuously records metric and reliability events:

* eventId
* siteId
* env
* timestamp
* eventType (regression, improvement, breakage, anomaly)
* metricKey (crawl_health, pages_losing_traffic, domain_authority, LCP, indexing_coverage, clicks, etc.)
* oldValue
* newValue
* delta
* severity (low/med/high)
* detectionSource (scheduler, monitor, manual flag)
* context (urls affected, error codes, affected templates)

3. AttributionRecord
   Created when the system attempts to link events to actions:

* attributionId
* siteId
* env
* eventId
* candidateActionIds[]
* timeProximityScore
* changeSurfaceScore (did the action touch the relevant surface?)
* historicalLikelihoodScore (based on prior similar cases)
* confounders[] (other possible causes: algo updates, seasonality, indexing delays)
* confidence (0–1)
* explanation (short narrative)

4. Socrates KB Entry (Learning)
   Created when attribution confidence is high enough:

* kbId
* title
* problemStatement (what happened)
* contextScope:

  * metricKey(s)
  * site archetype (local service, ecommerce, content site, etc.)
  * tech stack (if known)
  * page type (blog, landing, location page)
  * environment
* triggerPattern (what signals to look for)
* rootCauseHypothesis (what likely caused it)
* evidence:

  * eventIds[]
  * actionIds[]
  * diffs / commits / deployIds
  * beforeAfter snapshots
* recommendedAction (what to do next time)
* avoidAction (what not to do)
* guardrail (tests/checks to prevent repeats)
* confidence (0–1)
* status (draft, active, deprecated)
* createdAt / updatedAt
* tags (crawl, indexing, CWV, content, ads, etc.)

Detection & Learning Creation Workflow

Step 1: Detect significant events
Define thresholds per metric for “learning-worthy” changes. Examples:

* Breakage: error rate spikes, pages 5xx, blank dashboards, job failures
* Regression: crawl health drops > X points, CWV drops > X points, indexing coverage drops > X%
* Improvement: sustained improvement > X for Y days, DA jump, clicks increase beyond baseline

Each event is written to OutcomeEventLog.

Step 2: Collect candidate actions
For a given event, pull all AgentActionLog entries in the observation window:

* Standard window: last 24h
* Fast metrics (build errors, UI regressions): last 1–6h
* Slow metrics (indexing/DA): last 3–14 days, weighted by proximity

Step 3: Score attribution
Compute confidence using weighted factors:

* Time proximity: closer actions score higher
* Surface match: actions touching related surfaces score higher

  * Example: CWV regression → look for deploys, JS/CSS changes, image changes, third-party scripts
  * Example: indexing drop → robots, sitemap, canonical, meta noindex, GSC changes, route changes
* Historical priors: if similar actions previously caused similar events, boost confidence
* Confounders: if external causes are plausible (Google update, seasonality), reduce confidence

Output an AttributionRecord.

Step 4: Create a learning (KB entry)
When confidence >= threshold, create a KB entry:

* If confidence >= 0.8 → Active learning
* If confidence 0.6–0.79 → Draft learning (needs more confirmation)
* If confidence < 0.6 → No KB entry; store attribution only

Step 5: Merge / dedupe learnings
If a new learning matches an existing one (same trigger pattern + recommendation):

* Append evidence
* Increase confidence
* Update lastSeen
* If conflicting guidance, create a “disputed” state and require escalation

Retrieval & Enforcement Workflow (How Agents “Learn”)

Step 1: Preflight Socrates check (mandatory)
Before an agent executes any task that can affect the site (deploy, content changes, SEO settings, tracking changes), it must:

* Query Socrates with:

  * metricKey(s)
  * actionType
  * targets (urls/files)
  * site archetype + stack (if known)
* Retrieve top N relevant learnings (ranked by confidence + recency + scope match)

Step 2: Plan annotation
The agent must produce a plan that:

* References applicable KB entries by kbId
* Applies “avoidAction” constraints
* Adds required guardrails/tests from the learning
* If it wants to violate a learning, it must escalate (ask user / require approval)

Step 3: Post-action validation
After executing:

* Re-measure expectedImpact metrics
* Record outcomes
* If outcome matches expected improvement, mark learning as reinforced
* If outcome contradicts a learning, create a counter-learning (draft) and flag for review

User-Facing Mental Model (Simple)

* “Learnings” = the system’s memory of what worked, what broke, and why.
* “Socrates” = the librarian + judge that turns incidents and wins into rules and playbooks.
* Agents must “check the library” before acting.

Initial Implementation (MVP)
Start simple, then mature.

MVP Phase 1 (fast)

* Store AgentActionLog for every action (already mostly present)
* Store OutcomeEventLog for a limited set of metrics (crawl health, CWV, indexing, clicks)
* Attribution v1: time-window + surface match only
* KB entry creation for high-confidence cases only
* Retrieval: agent asks Socrates “any relevant learnings?” and gets top 5

Phase 2 (better)

* Add confounders + priors
* Add merging/deduping
* Add “disputed” state + escalation workflow
* Add guardrail tests enforcement hooks (pre-deploy checks)

Phase 3 (flywheel)

* Auto-generate “recommendedAction” drafts
* Add success playbooks (not just failures)
* Add per-site personalization (what breaks on this site specifically)

Operational Rules (to avoid bad learnings)

* Never create a learning from a single low-confidence correlation.
* Prefer fewer, higher quality learnings.
* Every learning must include evidence links (eventIds, actionIds).
* Learnings expire unless reconfirmed (e.g., decay confidence after 60 days without repeats).

Acceptance Criteria (What “Working” Looks Like)

* When something breaks, Socrates creates a KB entry with evidence and a recommended guardrail.
* Next time an agent tries a similar action, it retrieves the learning and either:

  * avoids the failure automatically, or
  * escalates with an explanation and safer options.
* Over time, the rate of repeated failures decreases and successful actions become standardized playbooks.
