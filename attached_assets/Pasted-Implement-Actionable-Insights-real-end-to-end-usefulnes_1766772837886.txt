Implement “Actionable Insights” + real end-to-end usefulness (EmpathyHealthClinic only)

Context / Goal
We have all workers connected and returning data. The app still feels like a status console. I need it to behave like a backend “SEO operator” that produces implementable insights and workflows, replacing placeholders across the UI with real data and giving me a ranked punch list of what to do next for empathyhealthclinic.com.

Constraints

* Single site for now: default DOMAIN = empathyhealthclinic.com (use existing site record / config).
* No placeholders: never show 0 or fake numbers; show real computed values or “N/A / Not connected”.
* Preserve existing worker orchestration; build on top of stored results in DB.
* Everything must be driven by persisted results so pages load fast and “Get more data” refreshes them.

Deliverable 1 — Unified “Run” + “Get more data” flow

1. Ensure there is one canonical run endpoint that the UI uses everywhere:

* POST /api/run (or keep POST /api/run/workers but wire UI buttons to it)
* It triggers all enabled workers for the active site and stores results.
* Response returns { ok:true, runId, siteId, domain } immediately.

2. Add run status polling:

* GET /api/run/:runId -> { status: queued|running|complete|partial|failed, workerStatuses, startedAt, finishedAt }

3. Update every “Get more data” button to:

* call POST /api/run (siteId inferred / selected site)
* show loading state
* poll until complete/partial
* refresh dashboard + pages by fetching “latest” endpoints (below)

Deliverable 2 — “Latest data” endpoints (fast UI)
Implement read endpoints that return the latest stored results for the current site:

* GET /api/latest/worker-results?siteId=...
* GET /api/latest/dashboard?siteId=...
* GET /api/latest/suggestions?siteId=...
* GET /api/latest/tickets?siteId=...
* GET /api/latest/kbase-insights?siteId=...
  These should read the latest completed run (or most recent partial run) and return normalized data for the UI.

Deliverable 3 — Normalize key metrics from worker payloads
Right now payloads are inconsistent. Add a normalization layer:

* For each worker_key, write a mapper that extracts a small metrics object:

  * serp_intel: tracked_keywords_count, top10_count, pos_11_20_count, avg_position, opportunities[]
  * technical_crawler: pages_checked, total_pages_discovered, issues_count_by_type, top_issues[]
  * core_web_vitals: lcp, inp, cls, failing_urls_count, slow_urls[]
  * backlinks/authority: authority_score, referring_domains, backlinks_total, new_links_30d, lost_links_30d
  * competitive_intel: competitors[], gaps[], new_pages_detected[]
  * content_decay_monitor: decayed_pages_count, decayed_pages[]
  * content_validator: failing_pages_count, violations_by_type
  * blog_writer: last_generated, queue_count (if applicable)
  * notifications: last_sent, channels_ok
    Store:
* raw payload_json (unchanged)
* normalized metrics_json (new)
* summary_text (human readable)

Deliverable 4 — Replace placeholders across the UI with real computed values
Find and remove all mock/placeholder values in:

* Dashboard cards
* Industry benchmarks panel
* Knowledge base insights panel
* SERP Tracking tables
* Technical SEO summary
  Rules:
* If data exists, show it.
* If missing, show “N/A” or “Not connected”, never 0.0.
* Avg position must never be 0.0. If no keywords, show “N/A”.
* Keyword Ranking must show TOP 100 (not 25). Update worker call / pagination to request/return up to 100.
* Technical SEO must show “Checked X of Y pages” where Y = total_pages_discovered (or sitemap count fallback).

Deliverable 5 — The core feature: Actionable Insights engine (ranked punch list)
Implement an insights/suggestions generator that runs after each run completes:

* Input: normalized metrics across workers for the run
* Output: a ranked list of suggestions with evidence and steps
  Persist to a table (or reuse existing) like seo_suggestions:
  Fields:
* id, runId, siteId, createdAt
* type (technical|content|serp|authority|cwv|competitive|decay)
* title
* severity (low|med|high)
* impact_score (0-100)
* evidence_json (metrics + urls + keywords)
* actions_json (step-by-step fix list)
* status (open|done|dismissed)
  Ranking:
* High severity + high impact first
* Prefer “quick wins” (positions 11–20, fixable technical blockers) near top

Minimum suggestion rules to implement (v1)
SERP Quick Wins:

* If pos_11_20_count > 0: create suggestion “Push X keywords into Top 10”
* Evidence: list top 10 keywords in 11–20 with landing pages
* Actions: on-page checklist + internal links to add + FAQ section suggestions
  Technical Blockers:
* If technical has missing titles/H1/duplicate meta/canonicals: create per-issue suggestion with URL list
  CWV:
* If failing_urls_count > 0: suggestion “Fix slow pages affecting CWV”
  Authority:
* If authority_score below threshold or lost_links_30d high: suggestion “Recover lost links / build authority”
  Decay:
* If decayed_pages_count > 0: suggestion “Refresh decayed pages”
  Competitive:
* If gaps exist: suggestion “Create missing pages for competitor gaps”

Deliverable 6 — “Fix Pack” actions (generate implementable outputs)
For each suggestion, add server endpoints that return generated fix artifacts (no UI placeholder):

* POST /api/suggestions/:id/fix-pack
  Returns:
* recommended_title/meta/H1 updates
* outline + FAQ + section headings
* internal links to add (from known site pages)
* draft copy blocks for the page
  Use the existing AI integration and include citations/evidence from the suggestion’s evidence_json.

Deliverable 7 — Knowledge Base Insights (real)
Implement:

* GET /api/kbase/insights/latest?siteId=...
  Generate 3–5 insights per run by summarizing:
* top suggestions
* recurring technical issues
* biggest keyword opportunities
  Persist them (seo_kbase_insights). If KBase write is enabled, also write/update KB articles via the KBase worker:
* Weekly SEO Summary for empathyhealthclinic.com
* Top Technical Issues
* Keyword Opportunities (11–20)
* Decay Watchlist
  Return links/ids to show in UI.

Deliverable 8 — Tickets (real + actionable)
Implement ticket generation from suggestions:

* “Engineering” tickets for CWV + technical crawler issues
* “Content” tickets for decay/validator/serp pages
  Persist tickets and wire UI actions:
* Mark done, dismiss, add note
  Ticket detail view must show evidence URLs and steps.

Deliverable 9 — Industry Benchmarks: fix or remove until sourced
If benchmarks are not actually sourced, do NOT show fake benchmark numbers.
Either:
A) Hide the benchmark comparison module entirely unless benchmark data exists, or
B) Show only “Your metrics” and label the rest “Benchmarks not loaded”
Under no circumstances display avg position 0.0.

Deliverable 10 — Acceptance checks

* After clicking “Run smoke tests” or “Get more data”, dashboard updates within the same session using stored latest run.
* Dashboard shows a ranked “Suggested Changes / Actionable Insights” list (minimum 10 items when data exists).
* KBase Insights shows 3–5 items (not empty) after a run.
* SERP Tracking shows 100 keywords (or all available up to 100).
* Technical SEO shows Checked X of Y pages and issue counts based on real crawler output.
* No placeholder numbers remain anywhere (search for “placeholder”, “TODO”, “mock”, hardcoded 25/20/0).
